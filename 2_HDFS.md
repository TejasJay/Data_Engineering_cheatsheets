# **ğŸ”¥ Step 1: Introduction to Hadoop & HDFS**

### **ğŸ“Œ What is HDFS?**

HDFS (**Hadoop Distributed File System**) is a **fault-tolerant, distributed file system** designed to store **large-scale data** across multiple machines.

âœ… **Why use HDFS?**

-   Handles **huge datasets** (terabytes to petabytes).
-   **Scalable** â€“ Adds more nodes as storage grows.
-   **Fault-tolerant** â€“ Replicates data across machines.
* * *

## **ğŸ”¥ Step 2: HDFS Components**

HDFS has a **Master-Slave Architecture**:

| **Component** | **Function** |
| --- | --- |
| **NameNode (Master)** | Manages metadata (file locations, permissions, replication info). |
| **DataNodes (Slaves)** | Store actual data blocks. |
| **Secondary NameNode** | Helps in metadata checkpointing (not a failover node). |

* * *

## **ğŸ”¥ Step 3: Install & Start HDFS**

### **âœ… 1. Format the NameNode (Prepares HDFS for First-Time Use)**

bash

CopyEdit

`hdfs namenode -format`

ğŸ“Œ **What does this do?**

-   Initializes the **metadata storage** of the NameNode.
-   Deletes old metadata and creates a **fresh file system**.
* * *

### **âœ… 2. Start HDFS Services**

bash

CopyEdit

`start-dfs.sh`

ğŸ“Œ **What does this do?**

-   Starts **NameNode (Master)** and **DataNodes (Workers)** in the cluster.
-   Allows files to be stored and retrieved in HDFS.

ğŸ”¹ **Verify services are running:**

bash

CopyEdit

`jps`

ğŸ“Œ **What does this show?**

-   Lists running Hadoop processes (`NameNode`, `DataNode`, `SecondaryNameNode`).
* * *

### **âœ… 3. Check HDFS Cluster Status**

bash

CopyEdit

`hdfs dfsadmin -report`

ğŸ“Œ **What does this do?**

-   Displays **live DataNodes, total storage, free space, and replication factor**.
* * *

# **ğŸ”¥ Step 4: Basic HDFS Commands (Hands-On)**

ğŸ“Œ **Now letâ€™s create directories, upload files, move, and delete files in HDFS.**

* * *

### **âœ… 1. Create Directories in HDFS**

bash

CopyEdit

`hdfs dfs -mkdir /user/your_username`

ğŸ“Œ **What does this do?**

-   Creates a new directory `/user/your_username` inside HDFS.
-   Similar to `mkdir` in Linux, but for HDFS.

bash

CopyEdit

`hdfs dfs -mkdir /user/your_username/input hdfs dfs -mkdir /user/your_username/output`

ğŸ“Œ **Why create multiple directories?**

-   `/input` â†’ Stores raw files.
-   `/output` â†’ Stores processed data.

ğŸ”¹ **Verify directories were created:**

bash

CopyEdit

`hdfs dfs -ls /user/your_username`

ğŸ“Œ **What does this do?**

-   Lists all files and directories inside `/user/your_username`.
* * *

### **âœ… 2. Upload Files to HDFS**

bash

CopyEdit

`echo "Hadoop is a distributed file system" > localfile.txt hdfs dfs -put localfile.txt /user/your_username/input/`

ğŸ“Œ **What does this do?**

-   Creates a sample file `localfile.txt` in local storage.
-   `put` uploads the file to HDFS under `/user/your_username/input/`.

ğŸ”¹ **Verify file was uploaded:**

bash

CopyEdit

`hdfs dfs -ls /user/your_username/input/`

ğŸ“Œ **What does this do?**

-   Lists all files in `/user/your_username/input/` inside HDFS.
* * *

### **âœ… 3. Read Files from HDFS**

bash

CopyEdit

`hdfs dfs -cat /user/your_username/input/localfile.txt`

ğŸ“Œ **What does this do?**

-   Reads and displays the content of `localfile.txt` stored inside HDFS.
-   Similar to `cat` in Linux.
* * *

### **âœ… 4. Download Files from HDFS**

bash

CopyEdit

`hdfs dfs -get /user/your_username/input/localfile.txt .`

ğŸ“Œ **What does this do?**

-   Retrieves `localfile.txt` from HDFS and saves it in the **current directory (`.`)**.

bash

CopyEdit

`hdfs dfs -copyToLocal /user/your_username/input/localfile.txt .`

ğŸ“Œ **Whatâ€™s the difference?**

-   Both `get` and `copyToLocal` copy files from HDFS to the local file system.
-   `get` provides **progress status**, while `copyToLocal` doesnâ€™t.
* * *

### **âœ… 5. Rename & Move Files in HDFS**

bash

CopyEdit

`hdfs dfs -mv /user/your_username/input/localfile.txt /user/your_username/input/data.txt`

ğŸ“Œ **What does this do?**

-   Renames `localfile.txt` to `data.txt`.

bash

CopyEdit

`hdfs dfs -mv /user/your_username/input/data.txt /user/your_username/output/`

ğŸ“Œ **What does this do?**

-   Moves `data.txt` from `/input/` to `/output/`.

ğŸ”¹ **Check if the file was moved:**

bash

CopyEdit

`hdfs dfs -ls /user/your_username/output/`

ğŸ“Œ **What does this do?**

-   Lists the contents of `/output/` to verify the move.
* * *

### **âœ… 6. Delete Files & Directories in HDFS**

bash

CopyEdit

`hdfs dfs -rm /user/your_username/output/data.txt`

ğŸ“Œ **What does this do?**

-   Deletes `data.txt` from HDFS.

bash

CopyEdit

`hdfs dfs -rm -r /user/your_username/output`

ğŸ“Œ **What does this do?**

-   Deletes **the entire directory `/output/`** and its contents.
* * *

# **ğŸ”¥ Step 5: File Permissions & ACLs in HDFS**

ğŸ“Œ **Every file in HDFS has permissions like Linux (`rwx`).**

### **âœ… 1. Check File Permissions**

bash

CopyEdit

`hdfs dfs -ls -h /user/your_username/input/`

ğŸ“Œ **What does this do?**

-   Lists files with **ownership, size, and permissions** in human-readable format (`-h`).
* * *

### **âœ… 2. Change File Permissions**

bash

CopyEdit

`hdfs dfs -chmod 755 /user/your_username/input/`

ğŸ“Œ **What does this do?**

-   Gives **read, write, and execute (rwx) permissions** to the owner.
-   Gives **read & execute (r-x) permissions** to others.
* * *

### **âœ… 3. Change File Ownership**

bash

CopyEdit

`hdfs dfs -chown your_user:your_group /user/your_username/input/`

ğŸ“Œ **What does this do?**

-   Changes **file owner and group ownership** in HDFS.
* * *

### **âœ… 4. Set ACLs for Fine-Grained Access**

bash

CopyEdit

`hdfs dfs -setfacl -m user:anotheruser:rwx /user/your_username/input/`

ğŸ“Œ **What does this do?**

-   Grants **full access (rwx)** to `anotheruser`.

bash

CopyEdit

`hdfs dfs -setfacl -b /user/your_username/input/`

ğŸ“Œ **What does this do?**

-   Removes all ACL rules from `/input/`.
* * *


# **ğŸ”¥ 1. How HDFS Stores Data (Block-Based Storage)**

### **ğŸ“Œ Why Blocks?**

HDFS **splits large files** into **fixed-size blocks** to store across multiple machines.

ğŸ“Œ **Default Block Size:**

-   **128MB** (Hadoop 2.x & 3.x, configurable)
-   **64MB** (Hadoop 1.x)

ğŸ“Œ **How File Storage Works in HDFS:** 1ï¸âƒ£ When a file is uploaded, HDFS **divides it into blocks** (e.g., a **600MB file** â†’ 5 blocks of 128MB each).
2ï¸âƒ£ Each block is stored on **different DataNodes** to ensure redundancy & fault tolerance.
3ï¸âƒ£ The **NameNode tracks where each block is stored** in metadata.

ğŸ’¡ **Example: A 500MB file in HDFS (128MB block size)**

| Block | DataNode 1 | DataNode 2 | DataNode 3 |
| --- | --- | --- | --- |
| Block 1 (128MB) | âœ… | âœ… (Replica) | âœ… (Replica) |
| Block 2 (128MB) | âœ… | âœ… (Replica) | âœ… (Replica) |
| Block 3 (128MB) | âœ… | âœ… (Replica) | âœ… (Replica) |
| Block 4 (116MB) | âœ… | âœ… (Replica) | âœ… (Replica) |

ğŸ“Œ **View Block Size Configuration in HDFS**

bash

CopyEdit

`hdfs getconf -confKey dfs.blocksize`

ğŸ”¹ **Displays the current block size setting in bytes (default: 134217728 for 128MB).**

ğŸ“Œ **View Block Details of a File**

bash

CopyEdit

`hdfs fsck /user/your_username/input/localfile.txt -files -blocks -locations`

ğŸ”¹ **Shows how HDFS splits a file into blocks and where each block is stored.**

* * *

# **ğŸ”¥ 2. HDFS NameNode & Metadata Management**

### **ğŸ“Œ What is Metadata?**

The **NameNode doesnâ€™t store actual data**â€”it only keeps a **mapping (metadata) of files to blocks**.

ğŸ“Œ **Metadata is stored in two files:**

| **Metadata File** | **Purpose** |
| --- | --- |
| **fsimage** | A snapshot of the HDFS namespace (file structure, permissions, locations). |
| **edits log** | Tracks recent metadata changes (file creation, deletion, renaming). |

ğŸ’¡ **Example: If you create a file in HDFSâ€¦**

-   NameNode **updates `edits log` immediately**.
-   Every few minutes, the **Secondary NameNode merges `fsimage` and `edits log`** into a new `fsimage` file.

ğŸ“Œ **View fsimage & edits log (Hands-On)**

bash

CopyEdit

`ls -lh /usr/local/hadoop/tmp/dfs/name/current/`

ğŸ”¹ **Shows metadata files (`fsimage`, `edits`, `VERSION`).**

ğŸ“Œ **Check Safe Mode Status (Read-Only Mode for Metadata)**

bash

CopyEdit

`hdfs dfsadmin -safemode get`

ğŸ”¹ **If enabled, HDFS doesnâ€™t allow writes until block reports are received.**

ğŸ“Œ **Manually Force a Checkpoint (Merge fsimage & edits log)**

bash

CopyEdit

`hdfs secondarynamenode -checkpoint force`

ğŸ”¹ **Forces the Secondary NameNode to merge metadata.**

* * *

# **ğŸ”¥ 3. HDFS Replication & Fault Tolerance**

### **ğŸ“Œ How HDFS Handles Failures (Replication Mechanism)**

âœ… By default, **each block is replicated 3 times** for fault tolerance.
âœ… If a **DataNode fails**, HDFS **automatically re-replicates lost blocks**.

ğŸ“Œ **Check the Replication Factor of a File**

bash

CopyEdit

`hdfs fsck /user/your_username/input/localfile.txt -files -blocks`

ğŸ”¹ **Displays the replication factor of each block.**

ğŸ“Œ **Manually Set Replication Factor**

bash

CopyEdit

`hdfs dfs -setrep -w 2 /user/your_username/input/localfile.txt`

ğŸ”¹ **Changes replication factor to 2 (default is 3).**

ğŸ“Œ **View Missing Blocks in HDFS**

bash

CopyEdit

`hdfs fsck / -missing`

ğŸ”¹ **Lists any missing blocks that need replication.**

* * *

# **ğŸ”¥ 4. Rack Awareness in HDFS**

### **ğŸ“Œ What is Rack Awareness?**

HDFS **distributes data across multiple racks** in a data center to **protect against entire rack failures**.

âœ… Blocks are stored in **different racks** to **ensure high availability**. âœ… Default policy: **One block replica in a different rack** to avoid total data loss.

ğŸ’¡ **Example: If a Data Center has 3 Racks (`Rack-1`, `Rack-2`, `Rack-3`)**

| Block | DataNode in Rack-1 | DataNode in Rack-2 | DataNode in Rack-3 |
| --- | --- | --- | --- |
| Block 1 | âœ… (Replica 1) | âœ… (Replica 2) | âœ… (Replica 3) |

ğŸ“Œ **Check Rack Awareness Configuration**

bash

CopyEdit

`hdfs getconf -confKey net.topology.script.file.name`

ğŸ”¹ **Shows the script used for rack topology.**

ğŸ“Œ **View Blocks Placement Across Racks**

bash

CopyEdit

`hdfs fsck / -files -blocks -racks`

ğŸ”¹ **Shows which racks store each file's blocks.**

* * *

# **ğŸ”¥ 5. Simulating HDFS Failures (Hands-On)**

### **âœ… 1. Kill a DataNode to Simulate Failure**

bash

CopyEdit

`jps  # Find the DataNode process ID kill -9 <DataNode_PID>`

ğŸ”¹ **Kills a DataNode to test HDFS self-healing.**

ğŸ“Œ **Check Missing Blocks After DataNode Failure**

bash

CopyEdit

`hdfs fsck / -files -blocks -locations`

ğŸ”¹ **Lists blocks that lost replication.**

ğŸ“Œ **Force HDFS to Rebalance & Restore Replication**

bash

CopyEdit

`hdfs balancer`

ğŸ”¹ **Redistributes blocks to ensure proper replication.**

* * *

## ğŸ”¹ **1\. NameNode & Metadata**

The **NameNode** is the **master node** that manages metadata and file system operations.

âœ… **Key Responsibilities**
ğŸ”¹ Maintains the **file system namespace** (directory structure).
ğŸ”¹ Stores **metadata** (mapping of file names to block locations).
ğŸ”¹ Manages **DataNodes** and keeps track of active nodes.
ğŸ”¹ Handles **replication** and ensures fault tolerance.

âœ… **Important Files in NameNode Storage**
ğŸ“‚ `fsimage` â€“ A snapshot of the entire file system metadata.
ğŸ“‚ `edits` â€“ A log of recent file system changes since the last `fsimage` save.
ğŸ“‚ `fstime` â€“ Stores the last checkpoint timestamp.

* * *

## ğŸ”¹ **2\. DataNodes & Storage**

The **DataNodes** are the **worker nodes** that store the actual data blocks.

âœ… **Key Responsibilities**
ğŸ”¹ Store **HDFS data blocks** on the local file system.
ğŸ”¹ Periodically send **heartbeat** signals to the NameNode.
ğŸ”¹ Perform **read/write operations** on request.
ğŸ”¹ Replicate data blocks based on the replication factor.

âœ… **How DataNodes Manage Data Blocks**
ğŸ“¦ Data is split into **blocks** (default: 128MB).
ğŸ” Each block is replicated **(default: 3 copies)** across multiple DataNodes.
ğŸ” The **Block Report** is sent to the NameNode to verify block locations.

* * *

## ğŸ”¹ **3\. HDFS Read & Write Process**

Understanding **how HDFS reads and writes files** is key to mastering its architecture.

âœ… **ğŸ“ Write Process**
1ï¸âƒ£ Client contacts **NameNode** to request file creation.
2ï¸âƒ£ NameNode **allocates blocks** and selects DataNodes for storage.
3ï¸âƒ£ Client **writes data** to the selected DataNodes in a pipeline.
4ï¸âƒ£ DataNodes replicate the blocks **(default: 3 copies)**.
5ï¸âƒ£ Once all blocks are written, the **NameNode updates metadata**.

âœ… **ğŸ“– Read Process**
1ï¸âƒ£ Client requests a file from the **NameNode**.
2ï¸âƒ£ NameNode returns a **list of DataNodes** storing the file blocks.
3ï¸âƒ£ Client reads the blocks **directly from DataNodes** in parallel.
4ï¸âƒ£ Once all blocks are retrieved, the **file is reconstructed**.

* * *

## ğŸ”¹ **4\. Rack Awareness**

HDFS follows a **rack-aware block placement policy** to improve fault tolerance.

âœ… **What is Rack Awareness?**
ğŸ”¹ HDFS **assigns racks** to DataNodes.
ğŸ”¹ NameNode ensures that **replicated blocks are placed on different racks**.
ğŸ”¹ This prevents data loss in case of **rack failure**.

âœ… **Default Replication Strategy (Replication Factor = 3)**
ğŸ“Œ Block 1 â†’ **Rack 1, Node A**
ğŸ“Œ Block 1 Copy â†’ **Rack 2, Node B**
ğŸ“Œ Block 1 Copy â†’ **Rack 1, Node C**

This ensures that at least **one copy exists on a different rack** for redundancy.

* * *

## ğŸ”¹ **5\. Key Configuration Files**

HDFS behavior is controlled by key configuration files:

ğŸ“‚ **core-site.xml**

-   Defines **HDFS URI** (e.g., `hdfs://localhost:9000/`)
-   Configures **I/O operations** like buffer size.

ğŸ“‚ **hdfs-site.xml**

-   Configures **replication factor**, block size, and NameNode directories.
-   Example property:

    xml

    CopyEdit

    `<property>   <name>dfs.replication</name>   <value>3</value> </property>`

ğŸ“‚ **yarn-site.xml**

-   Configures **YARN resource management**.

ğŸ“‚ **mapred-site.xml**

-   Defines **MapReduce settings**.
* * *

âœ… **Step 1: Check if HDFS is Running**
Run the following command to verify if **NameNode and DataNodes** are running:

bash

CopyEdit

`jps`

ğŸ‘‰ You should see processes like `NameNode`, `DataNode`, `SecondaryNameNode`, `ResourceManager`, and `NodeManager`.
âŒ If **NameNode is missing**, try:

bash

CopyEdit

`hdfs --daemon start namenode`

* * *

âœ… **Step 2: Check HDFS Health**

bash

CopyEdit

`hdfs dfsadmin -report`

ğŸ‘‰ This should list **available DataNodes, storage capacity, and replication info**.
ğŸ’¡ Look for **Number of DataNodes** (should be at least **1**).

* * *

âœ… **Step 3: Create a Directory in HDFS**
Letâ€™s create a directory named **test\_dir** in HDFS:

bash

CopyEdit

`hdfs dfs -mkdir /user/$(whoami)/test_dir`

ğŸ‘‰ Verify if it was created:

bash

CopyEdit

`hdfs dfs -ls /user/$(whoami)`

* * *

âœ… **Step 4: Upload a File to HDFS**
ğŸ“ Create a test file locally:

bash

CopyEdit

`echo "Hello, HDFS!" > testfile.txt`

ğŸ“¤ Upload it to HDFS:

bash

CopyEdit

`hdfs dfs -put testfile.txt /user/$(whoami)/test_dir/`

ğŸ‘‰ Verify if the file is in HDFS:

bash

CopyEdit

`hdfs dfs -ls /user/$(whoami)/test_dir/`

* * *

âœ… **Step 5: Read the File from HDFS**

bash

CopyEdit

`hdfs dfs -cat /user/$(whoami)/test_dir/testfile.txt`

ğŸ‘‰ You should see:

CopyEdit

`Hello, HDFS!`

* * *

âœ… **Step 6: Check Block Information**
To check where the file is stored in HDFS:

bash

CopyEdit

`hdfs fsck /user/$(whoami)/test_dir/testfile.txt -files -blocks -locations`

ğŸ‘‰ This will show **block details** and which DataNodes store them.

* * *

âœ… **Step 7: Delete the File and Directory**
ğŸ—‘ï¸ Remove the file:

bash

CopyEdit

`hdfs dfs -rm /user/$(whoami)/test_dir/testfile.txt`

ğŸ—‚ï¸ Remove the directory:

bash

CopyEdit

`hdfs dfs -rmdir /user/$(whoami)/test_dir`


